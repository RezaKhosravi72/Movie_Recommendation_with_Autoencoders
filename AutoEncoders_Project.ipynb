{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac165f48-88a0-4287-ba47-ab0913775f7b",
   "metadata": {},
   "source": [
    "# Movie Recommendation System with Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fbd12-387f-4569-ad4b-cb5f8f5d72dd",
   "metadata": {},
   "source": [
    "### Project Overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d36d7-1de4-41a1-92ea-0f7d1b832b4f",
   "metadata": {},
   "source": [
    "This project showcases the development of a sophisticated movie recommendation system using deep learning techniques. Stacked Autoencoders (SAEs) play a central role in this project, enabling the system to understand user preferences and provide personalized movie recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99aecd3-e41a-4e55-b01a-a8b0444e4b83",
   "metadata": {},
   "source": [
    "#### Key Components and Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a0b8a-1007-4d81-84ca-fd7b35a21283",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "\n",
    "- The project begins by importing and preparing data from the MovieLens dataset. While the MovieLens dataset is rich in movie-related data, it is not used in this project. Instead, the focus is on building a recommendation system based on user-movie interactions.\n",
    "\n",
    "2. Data Conversion and Preprocessing:\n",
    "\n",
    "- The user-movie interaction data is converted into Torch tensors, making it compatible with PyTorch, a popular deep learning framework.\n",
    "- Ratings are processed to represent binary ratings: 1 (liked) or 0 (not liked).\n",
    "\n",
    "3. Stacked Autoencoder Architecture:\n",
    "\n",
    "- A Stacked Autoencoder neural network is designed, comprising multiple layers: an input layer (movies) and several hidden layers.\n",
    "- The SAE's architecture is specifically tailored for capturing complex patterns in user-movie interactions.\n",
    "\n",
    "4. Training the SAE:\n",
    "\n",
    "- The SAE is trained using a combination of Mean Squared Error (MSE) loss and backpropagation.\n",
    "- During training, the SAE learns the underlying structure of user preferences, reducing the reconstruction error in representing user-movie interactions.\n",
    "\n",
    "5. Testing and Evaluation:\n",
    "\n",
    "- The trained SAE is tested using a separate test dataset to assess its performance.\n",
    "- The test loss is calculated to measure the accuracy of the recommendations, providing insights into the model's ability to predict user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c856f3-c5ed-4685-b394-d09dc06d0a86",
   "metadata": {},
   "source": [
    "#### Key Insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3ef80-afc5-42c6-a600-d91d0a4bdd49",
   "metadata": {},
   "source": [
    "- Stacked Autoencoders are powerful tools for modeling and understanding user preferences in recommendation systems.\n",
    "- The project demonstrates how deep learning techniques can be used to provide personalized recommendations, enhancing user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39700b1-b842-40d8-a569-60f2358c55fb",
   "metadata": {},
   "source": [
    "#### Dataset Source:\n",
    "\n",
    "The MovieLens dataset serves as the source for this project. Although the dataset contains extensive movie-related data, this project focuses on the user-movie interaction aspect. The MovieLens dataset is widely used in the field of recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db1a58-0149-471b-b81f-210ffb850248",
   "metadata": {},
   "source": [
    "On the whole, this project highlights the application of deep learning in the development of recommendation systems and showcases the capabilities of Stacked Autoencoders in understanding and predicting user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea2c6a-32c6-466d-92e6-99247176021e",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee98f40d-7fd9-437b-84ae-37c04e62a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddb39370-5824-4b84-8698-0b5ecdf00b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn                # the module to implement neural network\n",
    "import torch.nn.parallel             # for parallel computation\n",
    "import torch.optim as optim          # the optimizer\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable  # for stochastic gradient discent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc360d99-9349-48dd-9a71-49f14988a24c",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09497729-7082-472d-956e-118418cd6e33",
   "metadata": {},
   "source": [
    " the separator is '::'\n",
    " \n",
    " We use **encoding = latin-1** because some movie names have special characters.\n",
    " \n",
    " since our columns have no names, we add header = None.\n",
    " \n",
    " We will use just the first column which is the movie id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f13499bc-5753-486e-9c6f-6c9d32dbfda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78430635-a2f2-4dc6-965e-2547541b20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                   1                             2\n",
      "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4  5  Father of the Bride Part II (1995)                        Comedy\n",
      "(3883, 3)\n"
     ]
    }
   ],
   "source": [
    "print(movies.head())\n",
    "print(movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d01aee82-39f6-4d6f-b3ce-20fc74f6be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "# The first column is the user ID.\n",
    "# The second column corresponds to the movie ID.\n",
    "# The third column corresponds to the user's rating of that movie (1 to 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51111ea6-4fb4-412b-98ca-1a2ef170a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1  2          3\n",
      "0  1  1193  5  978300760\n",
      "1  1   661  3  978302109\n",
      "2  1   914  3  978301968\n",
      "3  1  3408  4  978300275\n",
      "4  1  2355  5  978824291\n",
      "(1000209, 4)\n"
     ]
    }
   ],
   "source": [
    "print(ratings.head())\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880bfff-ea0a-4a86-b437-a0d764fe2559",
   "metadata": {},
   "source": [
    "## Preparing the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10518705-eca1-4673-a8de-ecb6c843327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1  1.1  5  874965758\n",
      "0  1    2  3  876893171\n",
      "1  1    3  4  878542960\n",
      "2  1    4  3  876893119\n",
      "3  1    5  3  889751712\n",
      "4  1    7  4  875071561\n"
     ]
    }
   ],
   "source": [
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "print(training_set.head())\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')\n",
    "\n",
    "# In u1.base, the elements of each row are separated by a tab; hence delimiter = '\\t'\n",
    "# The 1st column is the user ID\n",
    "# The 2nd column is the movie ID\n",
    "# The 3rd column is the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87194dd9-a339-414a-9ae9-2257831f03af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79999, 4)\n",
      "(19999, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_set.shape)\n",
    "print(test_set.shape)\n",
    "# 80/20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de0a0d-7f17-4584-839e-29a19479e5b6",
   "metadata": {},
   "source": [
    "## Getting the number of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2ed2cf5-95e0-41ef-9f1b-eb61fb3d74ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 943\n",
      "number of movies: 1682\n"
     ]
    }
   ],
   "source": [
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "print('number of users:',nb_users)\n",
    "print('number of movies:', nb_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7299872-0122-425a-8cea-cab2c5bff4d5",
   "metadata": {},
   "source": [
    "## Converting the data into an array with users in lines and movies in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2e126a2-a261-4602-8b5b-b824d723b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a200c-164a-41a4-ac3b-341b19267de8",
   "metadata": {},
   "source": [
    "## Converting the data into Torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1253471-344a-401d-89a7-b4abb98f4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "553e145a-4a34-48ff-a35f-85cb2b8ac7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([943, 1682])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c80ce6c5-8ebb-4578-a7d4-2f06ed925e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([943, 1682])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b141a-b919-4ce0-b211-442cbc563ae3",
   "metadata": {},
   "source": [
    "## Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10062bc0-9ea5-484e-9985-705490f534db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module): # SAE class is an inheritance (child class) of an existing class named Module (parent class).\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()   # The super function is used to have access to the methods and functions of nn.Module\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)   # (num_movies)=f1=>>(20 nodes)=f2=>>(10 nodes)=f3=>>(20 nodes)=f4=>>(num_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5) # stochastic gradiant discent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937603c-860e-428b-ba3e-a2c17a52cbcf",
   "metadata": {},
   "source": [
    "## Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7eee0e3c-09a6-4113-9af3-c639f9b9b634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: tensor(1.7720)\n",
      "epoch: 2 loss: tensor(1.0967)\n",
      "epoch: 3 loss: tensor(1.0535)\n",
      "epoch: 4 loss: tensor(1.0383)\n",
      "epoch: 5 loss: tensor(1.0311)\n",
      "epoch: 6 loss: tensor(1.0265)\n",
      "epoch: 7 loss: tensor(1.0238)\n",
      "epoch: 8 loss: tensor(1.0220)\n",
      "epoch: 9 loss: tensor(1.0205)\n",
      "epoch: 10 loss: tensor(1.0197)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 10\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.   # It will count the number of users who rated at least one movie. [to extract those who did not rate any movie]\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0) # keras (pytorch) cannot accept a single vector of one dim. A batch of vec must be introduced.\n",
    "        target = input.clone()  # input will be modified later so a copy (clone) of it is made.\n",
    "        if torch.sum(target.data > 0) > 0:   # only those who rated at least one movie\n",
    "            output = sae(input)\n",
    "            target.require_grad = False   # We do not compute the grad-dis with respect to the target. \n",
    "            output[target == 0] = 0       # We do not want to predict the rates of the movies that the user did not rate. So when the rating is 0, it will remain 0 (not predicted)\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) \n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec9d55-b22d-420b-b979-5bbc11559e51",
   "metadata": {},
   "source": [
    "## Testing the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "572568ef-8431-4b18-a3e0-52f140312d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(1.0270)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "  input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "  target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "  if torch.sum(target.data > 0) > 0:\n",
    "    output = sae(input)\n",
    "    target.require_grad = False\n",
    "    output[target == 0] = 0\n",
    "    loss = criterion(output, target)\n",
    "    mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "    test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "    s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
